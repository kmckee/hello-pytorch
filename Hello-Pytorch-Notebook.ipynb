{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python + Pytorch Starter Notebook\n",
    "\n",
    "Notes from my adventure setting up a Jupyter Notebook for Pytorch on my mac.\n",
    "\n",
    "## Does python work?\n",
    "\n",
    "Select the cell below and hit Ctrl + Enter to execute.  I set up conda and had to configure the kernel.  ctrl-Shift-P Create Jupyter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is Pytorch installed and available in this kernel?\n",
    "\n",
    "Run this next:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1837, 0.8384, 0.7957],\n",
      "        [0.3421, 0.3699, 0.4904],\n",
      "        [0.4152, 0.1468, 0.4164],\n",
      "        [0.0644, 0.8416, 0.6187],\n",
      "        [0.2257, 0.6681, 0.1577]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you see the tensor output above, it's working!  \n",
    "\n",
    "## Using Pytorch\n",
    "\n",
    "Let's get some data, but first more imports.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download training data from open datasets.\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have some data to use for training.  Pass the dataset to a dataloader.  Dataloaders give you an iterable and handles things like shuffling/randomization for you.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
      "Shape of y: torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Model\n",
    "\n",
    "Let's create the model we will use.  The model is an object that represents the architecture of the network.  How many layers?  What kind of layers?  How do we move forward?\n",
    "\n",
    "After we create an instance of the model, we send it `to` the device it will be run on (CUDA if you've got a GPU capable, my macbook is on mps, nad worst case you'll run on CPU.  Performance gets worse as you move down the list but it still works.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing the model's Parameters\n",
    "\n",
    "The model needs to know how to calculate loss and an optimizer.  I need to understand what these do better, but I know you're basically selecting from some math functions based on how you think they'll perform with the rest of your model.  Experimentation to find what works best with your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "In this loop it trains itself and updates it's weights based on how it does.  We'll actually run it soon, when we write the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "            "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check our progress\n",
    "\n",
    "The other function we need to implement our training loop is a test function.  There's a bunch I need to read more about in here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Training loop!\n",
    "\n",
    "Here goes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.310679  [   64/60000]\n",
      "loss: 2.289166  [ 6464/60000]\n",
      "loss: 2.268306  [12864/60000]\n",
      "loss: 2.257291  [19264/60000]\n",
      "loss: 2.237663  [25664/60000]\n",
      "loss: 2.214821  [32064/60000]\n",
      "loss: 2.225413  [38464/60000]\n",
      "loss: 2.189602  [44864/60000]\n",
      "loss: 2.183145  [51264/60000]\n",
      "loss: 2.153429  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 49.6%, Avg loss: 2.139284 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.157562  [   64/60000]\n",
      "loss: 2.137975  [ 6464/60000]\n",
      "loss: 2.071871  [12864/60000]\n",
      "loss: 2.090625  [19264/60000]\n",
      "loss: 2.034302  [25664/60000]\n",
      "loss: 1.976987  [32064/60000]\n",
      "loss: 2.010672  [38464/60000]\n",
      "loss: 1.926161  [44864/60000]\n",
      "loss: 1.925032  [51264/60000]\n",
      "loss: 1.863808  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 59.0%, Avg loss: 1.846233 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.882257  [   64/60000]\n",
      "loss: 1.842091  [ 6464/60000]\n",
      "loss: 1.715456  [12864/60000]\n",
      "loss: 1.772566  [19264/60000]\n",
      "loss: 1.659845  [25664/60000]\n",
      "loss: 1.613323  [32064/60000]\n",
      "loss: 1.645591  [38464/60000]\n",
      "loss: 1.549444  [44864/60000]\n",
      "loss: 1.565287  [51264/60000]\n",
      "loss: 1.475938  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.8%, Avg loss: 1.481551 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.545654  [   64/60000]\n",
      "loss: 1.508643  [ 6464/60000]\n",
      "loss: 1.354181  [12864/60000]\n",
      "loss: 1.442541  [19264/60000]\n",
      "loss: 1.327989  [25664/60000]\n",
      "loss: 1.317163  [32064/60000]\n",
      "loss: 1.339835  [38464/60000]\n",
      "loss: 1.274173  [44864/60000]\n",
      "loss: 1.297867  [51264/60000]\n",
      "loss: 1.208549  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 1.227636 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.298206  [   64/60000]\n",
      "loss: 1.281876  [ 6464/60000]\n",
      "loss: 1.113403  [12864/60000]\n",
      "loss: 1.230139  [19264/60000]\n",
      "loss: 1.114030  [25664/60000]\n",
      "loss: 1.124063  [32064/60000]\n",
      "loss: 1.153277  [38464/60000]\n",
      "loss: 1.102588  [44864/60000]\n",
      "loss: 1.130456  [51264/60000]\n",
      "loss: 1.054040  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 65.5%, Avg loss: 1.069210 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wicked cool - lets save it\n",
    "\n",
    "We've got a model.  It's not great - plenty of room to learn and improve the existing code.  Let's save the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved PyTorch Model State to model.pth\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "print(\"Saved PyTorch Model State to model.pth\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It saved the file next to this notebook file, it's in the explorer panel."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the model\n",
    "\n",
    "We can load an existing, saved model from disk like so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NeuralNetwork()\n",
    "model.load_state_dict(torch.load(\"model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: \"Ankle boot\", Actual: \"Ankle boot\"\n"
     ]
    }
   ],
   "source": [
    "# Next lets use it - and disable backpropagation.\n",
    "\n",
    "classes = [\n",
    "    \"T-shirt/top\",\n",
    "    \"Trouser\",\n",
    "    \"Pullover\",\n",
    "    \"Dress\",\n",
    "    \"Coat\",\n",
    "    \"Sandal\",\n",
    "    \"Shirt\",\n",
    "    \"Sneaker\",\n",
    "    \"Bag\",\n",
    "    \"Ankle boot\",\n",
    "]\n",
    "\n",
    "model.eval()\n",
    "x, y = test_data[0][0], test_data[0][1]\n",
    "with torch.no_grad():\n",
    "    pred = model(x)\n",
    "    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
    "    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kyle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
